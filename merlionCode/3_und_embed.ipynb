{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing /export/c09/lavanya/languageIdentification/zinglish/large/embeddingLarge/embed_1728062711.h5...\n",
      "Dataset: LID\n",
      "Data Shape: (200,)\n",
      "Dataset: Layer_1\n",
      "Data Shape: (315, 200, 192)\n",
      "Dataset: Layer_2\n",
      "Data Shape: (315, 200, 256)\n",
      "Dataset: Layer_3\n",
      "Data Shape: (315, 200, 384)\n",
      "Dataset: Layer_4\n",
      "Data Shape: (315, 200, 512)\n",
      "Dataset: Layer_5\n",
      "Data Shape: (315, 200, 384)\n",
      "Dataset: Layer_6\n",
      "Data Shape: (315, 200, 256)\n",
      "Parsing /export/c09/lavanya/languageIdentification/zinglish/large/embeddingLarge/embed_1728062733.h5...\n",
      "Dataset: LID\n",
      "Data Shape: (200,)\n",
      "Dataset: Layer_1\n",
      "Data Shape: (292, 200, 192)\n",
      "Dataset: Layer_2\n",
      "Data Shape: (292, 200, 256)\n",
      "Dataset: Layer_3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     21\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, filename)\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mparse_h5_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mparse_h5_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m h5_file\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mh5_file\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.8/site-packages/h5py/_hl/dataset.py:758\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fast_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    760\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "# Define the path where the H5 files are located\n",
    "path = \"/export/c09/lavanya/languageIdentification/zinglish/large/embeddingLarge\"\n",
    "\n",
    "# Function to parse an H5 file\n",
    "def parse_h5_file(file_path):\n",
    "    with h5py.File(file_path, 'r') as h5_file:\n",
    "        print(f\"Parsing {file_path}...\")\n",
    "        # Access dataset keys in the H5 file\n",
    "        for key in h5_file.keys():\n",
    "            print(f\"Dataset: {key}\")\n",
    "            data = h5_file[key][:]\n",
    "            print(f\"Data Shape: {data.shape}\")\n",
    "            #print(f\"Data Sample: {data[:5]}\")  # Print the first 5 entries as a sample\n",
    "\n",
    "# Loop over each file in the directory and parse if it's an H5 file\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith('.h5'):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        parse_h5_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, hdf5_file_paths, layer):\n",
    "        self.hdf5_file_paths = hdf5_file_paths  \n",
    "        self.layer = layer\n",
    "        self.num_files = sum(h5py.File(file, 'r')[self.layer].shape[1] for file in hdf5_file_paths)  \n",
    "        # print(\"num_files\",self.num_files)\n",
    "        # #num_files 49672\n",
    "        # print(\"len of hdf5_file_paths\",len(self.hdf5_file_paths))\n",
    "        # #len of hdf5_file_paths 249\n",
    "        # print(\"layer\",self.layer)\n",
    "        # #layer Layer_3\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_files\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        current_idx = idx\n",
    "        for hdf5_file_path in self.hdf5_file_paths:\n",
    "            with h5py.File(hdf5_file_path, 'r') as hf:\n",
    "                if current_idx < hf[self.layer].shape[1]:  \n",
    "                    X = hf[self.layer][:, current_idx, :]  \n",
    "                    y = hf['LID'][current_idx].decode('utf-8')\n",
    "                    break\n",
    "                current_idx -= hf[self.layer].shape[1]  \n",
    "        X = torch.tensor(X, dtype=torch.float32) \n",
    "        label_map = {'English': 0, 'Mandarin': 1}  \n",
    "        y = label_map[y]  \n",
    "        # print(\"_______________________in get item______________________________\")\n",
    "        # print(\"in get item X\",X)\n",
    "        # print(\"in get item X shape\",X.shape)\n",
    "        # print(\"in get item y\",y)\n",
    "        # print(\"in get item y shape\",y)\n",
    "        return X, y\n",
    "\n",
    "def collate_fn(batch):\n",
    "    X, y = zip(*batch)\n",
    "    X_padded = pad_sequence(X, batch_first=True)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "    print(\"_______________________in collate ______________________________\")\n",
    "    # print(\"X_padded\",X_padded)\n",
    "    print(\"X_padded shape\",X_padded.shape)\n",
    "    print(\"y_tensor\",y_tensor)\n",
    "    print(\"y_tensor shape\",y_tensor)\n",
    "\n",
    "    return X_padded, y_tensor\n",
    "\n",
    "def main(hdf5_dir, save_dir_plot, layer, batch_size):\n",
    "    layer_dir_plot = os.path.join(save_dir_plot, layer)\n",
    "    hdf5_file_paths = [os.path.join(hdf5_dir, f) for f in os.listdir(hdf5_dir) if f.endswith('.h5')]\n",
    "    #print(\"len of hdf5_file_paths is \", len(hdf5_file_paths))\n",
    "    #len of hdf5_file_paths is  249\n",
    "\n",
    "   \n",
    "    dataset = HDF5Dataset(hdf5_file_paths, layer)\n",
    "    # only calls get item\n",
    "\n",
    "    #print(\"dataset length is\",len(dataset))\n",
    "    #dataset length is 49672\n",
    "    # print(\"type of datset\", type(dataset))\n",
    "    # #type of datset <class '__main__.HDF5Dataset'>\n",
    "    # print(\"\\nInspecting the first data sample:\")\n",
    "    # print(\"dataset[0] is \", dataset[0])\n",
    "\n",
    "    '''\n",
    "    dataset[0] is  (tensor([[ 0.3550,  0.0613, -0.3587,  ...,  0.5942, -0.3976,  0.0628],\n",
    "        [ 0.3108, -0.1092,  0.0022,  ...,  0.2336, -0.6032,  0.2023],\n",
    "        [ 0.5742, -0.2064, -0.2224,  ...,  0.2964, -0.9036,  0.1687],\n",
    "        ...,\n",
    "        [ 0.1466, -0.0193, -0.1113,  ..., -0.3567, -0.3011, -0.0782],\n",
    "        [ 0.1592, -0.0165, -0.1007,  ..., -0.3558, -0.2960, -0.0840],\n",
    "        [ 0.1399,  0.0355, -0.0534,  ..., -0.3566, -0.2699, -0.0644]]), 0)\n",
    "    '''\n",
    "    # print(\"shape is dataset[0][0] is \", dataset[0][0].shape)\n",
    "    # # shape is dataset[0][0] is  torch.Size([315, 192])\n",
    "    # print(\"len of datset[0] is\",len(dataset[0]))\n",
    "    # # 2\n",
    "    # print(\"datset[0][1] is\",dataset[0][1])\n",
    "    # # 0\n",
    "\n",
    "\n",
    "    # print(\"\\nInspecting the ninth data sample:\")\n",
    "    # print(\"dataset[9] is \", dataset[9])\n",
    "\n",
    "    '''\n",
    "    (tensor([[ 0.2362,  0.0080, -0.4423,  ...,  0.5518, -0.3113, -0.0182],\n",
    "        [-0.1450, -0.2879, -0.3316,  ...,  0.3387, -0.5585, -0.0862],\n",
    "        [ 0.1668, -0.4250, -0.3199,  ...,  0.3697, -0.6664, -0.1377],\n",
    "        ...,\n",
    "        [ 0.0709, -0.0845, -0.1716,  ..., -0.3089, -0.3951, -0.1136],\n",
    "        [ 0.0850, -0.0711, -0.1694,  ..., -0.3221, -0.3827, -0.1115],\n",
    "        [ 0.0607,  0.0858, -0.1421,  ..., -0.3824, -0.2395, -0.0459]]), 0)\n",
    "    '''\n",
    "    # print(\"shape of dataset[8][0] is \", dataset[200][0].shape)\n",
    "    # # shape of dataset[9][0] is  torch.Size([315, 192])\n",
    "    # print(\"len of datset[9] is\",len(dataset[9]))\n",
    "    # # 2\n",
    "    # print(\"datset[9][1] is\",dataset[9][1])\n",
    "    # # 0\n",
    "\n",
    "    print(\"*****************************************************************\")\n",
    "    print(\"starting train_loader\")\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "    # calls collate_fn and get item\n",
    "    #print(\"train_loader\",train_loader)\n",
    "    #train_loader <torch.utils.data.dataloader.DataLoader object at 0x7fe5214d46a0>\n",
    "\n",
    "    print(\"\\nInspecting the first batch of data:\")\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        print(\"Batch X:\", batch_X)  # Display the first batch of embeddings\n",
    "        print(\"Batch X shape:\", batch_X.shape)  # Display the first batch of embeddings\n",
    "        print(\"Batch y:\", batch_y)    # Display the corresponding labels\n",
    "        print(\"Batch y shape:\", batch_y.shape)    # Display the corresponding labels\n",
    "        break  # Exit the loop after the first iteration\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Train a language identification model.')\n",
    "    parser.add_argument('--hdf5_dir', type=str, default='/export/c09/lavanya/languageIdentification/zinglish/large/embeddingLarge/', help='Path to the HDF5 dir')\n",
    "    parser.add_argument('--save_dir_plot', type=str, default='/export/c09/lavanya/languageIdentification/zinglish/large/compareLarge', help='save plots')\n",
    "    # change here for layer\n",
    "    parser.add_argument('--layer', type=str, default='Layer_1', help='Layer to include (e.g., Layer_3).')\n",
    "    parser.add_argument('--batch_size', type=int, default=10, help='Batch size for training (default: 32).')\n",
    "    args = parser.parse_args()\n",
    "    main(args.hdf5_dir, args.save_dir_plot, args.layer, args.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing /export/c09/lavanya/languageIdentification/seame/embed/conversation/embed_1732049581.h5...\n",
      "Dataset: LID\n",
      "Data Shape: (128,)\n",
      "Dataset: Layer_1\n",
      "Data Shape: (233, 128, 192)\n",
      "Dataset: Layer_2\n",
      "Data Shape: (233, 128, 256)\n",
      "Dataset: Layer_3\n",
      "Data Shape: (233, 128, 384)\n",
      "Dataset: Layer_4\n",
      "Data Shape: (233, 128, 512)\n",
      "Dataset: Layer_5\n",
      "Data Shape: (233, 128, 384)\n",
      "Dataset: Layer_6\n",
      "Data Shape: (233, 128, 256)\n",
      "Parsing /export/c09/lavanya/languageIdentification/seame/embed/conversation/embed_1732049679.h5...\n",
      "Dataset: LID\n",
      "Data Shape: (128,)\n",
      "Dataset: Layer_1\n",
      "Data Shape: (224, 128, 192)\n",
      "Dataset: Layer_2\n",
      "Data Shape: (224, 128, 256)\n",
      "Dataset: Layer_3\n",
      "Data Shape: (224, 128, 384)\n",
      "Dataset: Layer_4\n",
      "Data Shape: (224, 128, 512)\n",
      "Dataset: Layer_5\n",
      "Data Shape: (224, 128, 384)\n",
      "Dataset: Layer_6\n",
      "Data Shape: (224, 128, 256)\n",
      "Parsing /export/c09/lavanya/languageIdentification/seame/embed/conversation/embed_1732049768.h5...\n",
      "Dataset: LID\n",
      "Data Shape: (128,)\n",
      "Dataset: Layer_1\n",
      "Data Shape: (201, 128, 192)\n",
      "Dataset: Layer_2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     21\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, filename)\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mparse_h5_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m, in \u001b[0;36mparse_h5_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m h5_file\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mh5_file\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/capstone/lib/python3.8/site-packages/h5py/_hl/dataset.py:758\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fast_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    760\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "# Define the path where the H5 files are located\n",
    "path = \"/export/c09/lavanya/languageIdentification/seame/embed/conversation\"\n",
    "\n",
    "# Function to parse an H5 file\n",
    "def parse_h5_file(file_path):\n",
    "    with h5py.File(file_path, 'r') as h5_file:\n",
    "        print(f\"Parsing {file_path}...\")\n",
    "        # Access dataset keys in the H5 file\n",
    "        for key in h5_file.keys():\n",
    "            print(f\"Dataset: {key}\")\n",
    "            data = h5_file[key][:]\n",
    "            print(f\"Data Shape: {data.shape}\")\n",
    "            #print(f\"Data Sample: {data[:5]}\")  # Print the first 5 entries as a sample\n",
    "\n",
    "# Loop over each file in the directory and parse if it's an H5 file\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith('.h5'):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        parse_h5_file(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
